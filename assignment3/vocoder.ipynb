{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36815954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from t2spec_converter import TextToSpecConverter\n",
    "import soundfile as sf\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283f8e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0165aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessedLJSpeech(Dataset):\n",
    "    def __init__(self, root=\"./\"):\n",
    "        self.dataset = torchaudio.datasets.LJSPEECH(root=root, download=True)\n",
    "        self.sample_rate = 22050\n",
    "        self.mel_transform = MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=1024,\n",
    "            win_length=1024,\n",
    "            hop_length=256,\n",
    "            n_mels=80,\n",
    "            f_min=0.0,\n",
    "            f_max=8000.0,\n",
    "            power=1.5,\n",
    "        )\n",
    "        self.amplitude_to_db = AmplitudeToDB(stype=\"power\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr, transcript, *_ = self.dataset[idx]\n",
    "        assert sr == self.sample_rate, \"Sample rate mismatch\"\n",
    "        \n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform[0:1]\n",
    "\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        mel_spec_db = self.amplitude_to_db(mel_spec)\n",
    "        mel_spec_db = mel_spec_db.squeeze(0)  # [n_mels, T]\n",
    "        \n",
    "        return mel_spec_db, waveform.squeeze(0), transcript\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11694a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mel shape: torch.Size([8, 80, 824])\n",
      "Audio shape: torch.Size([8, 210845])\n",
      "Text: (\"Observing his blood-covered chest as he was pulled into his wife's lap, Governor Connally believed himself mortally wounded.\", 'A simultaneous attack was made upon the captain and the first mate.')\n"
     ]
    }
   ],
   "source": [
    "dataset = PreprocessedLJSpeech()\n",
    "\n",
    "# Даталоадер\n",
    "def collate_fn(batch):\n",
    "    # batch: list of tuples (mel_spec, audio, transcript)\n",
    "    mel_specs, audios, transcripts = zip(*batch)\n",
    "    \n",
    "    # паддинг мел-спектрограмм и аудио для батча (если нужно)\n",
    "    mel_lengths = [m.shape[1] for m in mel_specs]\n",
    "    max_mel_len = max(mel_lengths)\n",
    "    \n",
    "    padded_mels = []\n",
    "    for m in mel_specs:\n",
    "        pad = max_mel_len - m.shape[1]\n",
    "        if pad > 0:\n",
    "            m = torch.nn.functional.pad(m, (0, pad))\n",
    "        padded_mels.append(m)\n",
    "    mel_batch = torch.stack(padded_mels)\n",
    "    \n",
    "    # Аналогично с аудио (для удобства)\n",
    "    audio_lengths = [a.shape[0] for a in audios]\n",
    "    max_audio_len = max(audio_lengths)\n",
    "    padded_audios = []\n",
    "    for a in audios:\n",
    "        pad = max_audio_len - a.shape[0]\n",
    "        if pad > 0:\n",
    "            a = torch.nn.functional.pad(a, (0, pad))\n",
    "        padded_audios.append(a)\n",
    "    audio_batch = torch.stack(padded_audios)\n",
    "    \n",
    "    return mel_batch, audio_batch, transcripts\n",
    "\n",
    "# Пример DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Цикл проверки\n",
    "for mel_batch, audio_batch, transcripts in dataloader:\n",
    "    print(\"Mel shape:\", mel_batch.shape)\n",
    "    print(\"Audio shape:\", audio_batch.shape)\n",
    "    print(\"Text:\", transcripts[:2])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efef8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, upsample_factors=(4, 4, 4), n_mels=80):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(n_mels, 512, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(\n",
    "                512,\n",
    "                256,\n",
    "                kernel_size=upsample_factors[0] * 2,\n",
    "                stride=upsample_factors[0],\n",
    "                padding=upsample_factors[0] // 2 + 1,\n",
    "                output_padding=1,\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(\n",
    "                256,\n",
    "                128,\n",
    "                kernel_size=upsample_factors[1] * 2,\n",
    "                stride=upsample_factors[1],\n",
    "                padding=upsample_factors[1] // 2 + 1,\n",
    "                output_padding=1,\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(\n",
    "                128,\n",
    "                64,\n",
    "                kernel_size=upsample_factors[2] * 2,\n",
    "                stride=upsample_factors[2],\n",
    "                padding=upsample_factors[2] // 2 + 1,\n",
    "                output_padding=1,\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 1, kernel_size=7, padding=3),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, mel):\n",
    "        return self.model(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a13e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, 15, stride=1, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, 15, stride=4, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 256, kernel_size=15, stride=4, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(256, 512, kernel_size=15, stride=4, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(512, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio):\n",
    "        out = self.model(audio)\n",
    "        return out.mean(dim=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37c6aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(d_fake, gen_audio, real_audio):\n",
    "    l1 = F.l1_loss(gen_audio, real_audio)\n",
    "    adv = F.mse_loss(d_fake, torch.ones_like(d_fake))\n",
    "    return l1 + 0.001 * adv  \n",
    "\n",
    "def discriminator_loss(d_real, d_fake):\n",
    "    real_loss = F.mse_loss(d_real, torch.ones_like(d_real))\n",
    "    fake_loss = F.mse_loss(d_fake, torch.zeros_like(d_fake))\n",
    "    return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6bf0757",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "generator = Generator()\n",
    "generator.to(device)\n",
    "\n",
    "discriminator = Discriminator()\n",
    "discriminator.to(device)\n",
    "\n",
    "g_opt = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n",
    "d_opt = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f672928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Odmen\\AppData\\Local\\Temp\\ipykernel_13824\\1587640347.py:2: UserWarning: Using a target size (torch.Size([8, 189853])) that is different to the input size (torch.Size([8, 1, 47467])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  l1 = F.l1_loss(gen_audio, real_audio)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (47467) must match the size of tensor b (189853) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m gen_audio \u001b[38;5;241m=\u001b[39m generator(mel)\n\u001b[0;32m      8\u001b[0m d_fake \u001b[38;5;241m=\u001b[39m discriminator(gen_audio)\n\u001b[1;32m----> 9\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_fake\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m g_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m g_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m, in \u001b[0;36mgenerator_loss\u001b[1;34m(d_fake, gen_audio, real_audio)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerator_loss\u001b[39m(d_fake, gen_audio, real_audio):\n\u001b[1;32m----> 2\u001b[0m     l1 \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     adv \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(d_fake, torch\u001b[38;5;241m.\u001b[39mones_like(d_fake))\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m l1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m adv\n",
      "File \u001b[1;32mc:\\_Experiments\\GenSpeech_2025\\venv\\lib\\site-packages\\torch\\nn\\functional.py:3810\u001b[0m, in \u001b[0;36ml1_loss\u001b[1;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3808\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3810\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n",
      "File \u001b[1;32mc:\\_Experiments\\GenSpeech_2025\\venv\\lib\\site-packages\\torch\\functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (47467) must match the size of tensor b (189853) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for mel, real_audio, _ in dataloader:\n",
    "        mel = mel.to(device)\n",
    "        real_audio = real_audio.to(device)\n",
    "\n",
    "        # Generator\n",
    "        gen_audio = generator(mel)\n",
    "        d_fake = discriminator(gen_audio)\n",
    "        g_loss = generator_loss(d_fake, gen_audio, real_audio)\n",
    "        g_opt.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_opt.step()\n",
    "\n",
    "        # Discriminator\n",
    "        d_real = discriminator(real_audio)\n",
    "        d_fake = discriminator(gen_audio.detach())\n",
    "        d_loss = discriminator_loss(d_real, d_fake)\n",
    "        d_opt.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_opt.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch} | G_loss: {g_loss.item():.4f} | D_loss: {d_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2s = TextToSpecConverter()\n",
    "\n",
    "\n",
    "generator.load_state_dict(torch.load(\"generator.pt\"))\n",
    "generator.eval()\n",
    "\n",
    "with open(\"test_sentences.txt\") as f:\n",
    "    sentences = [line.strip() for line in f.readlines()]\n",
    "\n",
    "for i, sent in enumerate(sentences[:5]):\n",
    "    mel = torch.tensor(t2s.text2spec(sent)).unsqueeze(0).to(device)  # [1, 80, T]\n",
    "    with torch.no_grad():\n",
    "        audio = generator(mel).cpu().squeeze().numpy()\n",
    "    sf.write(f\"gen_{i}.wav\", audio, samplerate=22050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
